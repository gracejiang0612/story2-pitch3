{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI Data Journalist Analysis: Pedestrian Ramp Complaints\n",
        "# Following systematic analysis rules for story discovery\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better data viewing\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"=== AI DATA JOURNALIST ANALYSIS ===\")\n",
        "print(\"Dataset: Pedestrian Ramp Complaints\")\n",
        "print(\"Analysis Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: INITIAL DATA ASSESSMENT\n",
        "print(\"STEP 1: INITIAL DATA ASSESSMENT\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Load and validate the dataset\n",
        "try:\n",
        "    df = pd.read_csv('Pedestrian_Ramp_Complaints_20250708.csv')\n",
        "    print(\"‚úì Dataset loaded successfully\")\n",
        "    print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"COLUMN ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Identify all columns and their data types\n",
        "print(\"Column Names and Data Types:\")\n",
        "for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
        "    print(f\"{i:2d}. {col:<30} | {dtype}\")\n",
        "\n",
        "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "}).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "print(\"Missing values by column:\")\n",
        "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
        "\n",
        "if missing_summary['Missing_Count'].sum() == 0:\n",
        "    print(\"‚úì No missing values found in dataset\")\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*40)\n",
        "print(\"First few rows:\")\n",
        "print(df.head(3))\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NUMERICAL COLUMNS ANALYSIS\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"NUMERICAL STATISTICS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "if numerical_cols:\n",
        "    print(\"\\nBasic Statistics for Numerical Columns:\")\n",
        "    print(df[numerical_cols].describe())\n",
        "    \n",
        "    # Additional statistics\n",
        "    print(\"\\nAdditional Statistics:\")\n",
        "    for col in numerical_cols:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Range: {df[col].min()} to {df[col].max()}\")\n",
        "        print(f\"  Std Dev: {df[col].std():.2f}\")\n",
        "        print(f\"  Variance: {df[col].var():.2f}\")\n",
        "        print(f\"  Skewness: {df[col].skew():.2f}\")\n",
        "        print(f\"  Kurtosis: {df[col].kurtosis():.2f}\")\n",
        "else:\n",
        "    print(\"No numerical columns found for statistical analysis.\")\n",
        "\n",
        "# Check for date columns\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"TEMPORAL COVERAGE ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Look for potential date columns\n",
        "date_cols = []\n",
        "for col in df.columns:\n",
        "    if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'updated', 'submitted']):\n",
        "        date_cols.append(col)\n",
        "\n",
        "print(f\"Potential date columns found: {date_cols}\")\n",
        "\n",
        "# Try to parse dates and analyze temporal coverage\n",
        "for col in date_cols:\n",
        "    try:\n",
        "        # Try different date parsing methods\n",
        "        df[f'{col}_parsed'] = pd.to_datetime(df[col], errors='coerce')\n",
        "        valid_dates = df[f'{col}_parsed'].dropna()\n",
        "        \n",
        "        if len(valid_dates) > 0:\n",
        "            print(f\"\\n{col} temporal analysis:\")\n",
        "            print(f\"  Date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
        "            print(f\"  Time span: {(valid_dates.max() - valid_dates.min()).days} days\")\n",
        "            print(f\"  Valid dates: {len(valid_dates)}/{len(df)} ({len(valid_dates)/len(df)*100:.1f}%)\")\n",
        "            \n",
        "            # Yearly breakdown\n",
        "            if len(valid_dates) > 0:\n",
        "                yearly_counts = valid_dates.dt.year.value_counts().sort_index()\n",
        "                print(f\"  Years covered: {yearly_counts.index.min()} - {yearly_counts.index.max()}\")\n",
        "                print(\"  Yearly distribution:\")\n",
        "                for year, count in yearly_counts.items():\n",
        "                    print(f\"    {year}: {count} records\")\n",
        "        else:\n",
        "            print(f\"  No valid dates found in {col}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error parsing {col}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: OVERALL TREND ANALYSIS\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 2: OVERALL TREND ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Find the best date column for trend analysis\n",
        "main_date_col = None\n",
        "for col in date_cols:\n",
        "    if f'{col}_parsed' in df.columns:\n",
        "        valid_dates = df[f'{col}_parsed'].dropna()\n",
        "        if len(valid_dates) > len(df) * 0.5:  # Use column with >50% valid dates\n",
        "            main_date_col = f'{col}_parsed'\n",
        "            break\n",
        "\n",
        "if main_date_col:\n",
        "    print(f\"Using {main_date_col.replace('_parsed', '')} for trend analysis\")\n",
        "    \n",
        "    # Create time series analysis\n",
        "    df_with_dates = df[df[main_date_col].notna()].copy()\n",
        "    df_with_dates['year'] = df_with_dates[main_date_col].dt.year\n",
        "    df_with_dates['month'] = df_with_dates[main_date_col].dt.month\n",
        "    df_with_dates['quarter'] = df_with_dates[main_date_col].dt.quarter\n",
        "    df_with_dates['weekday'] = df_with_dates[main_date_col].dt.day_name()\n",
        "    \n",
        "    # Monthly trend analysis\n",
        "    monthly_counts = df_with_dates.groupby([df_with_dates[main_date_col].dt.to_period('M')]).size()\n",
        "    yearly_counts = df_with_dates.groupby('year').size()\n",
        "    \n",
        "    print(\"\\nüìà YEARLY TRENDS:\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    if len(yearly_counts) > 1:\n",
        "        for i, (year, count) in enumerate(yearly_counts.items()):\n",
        "            if i > 0:\n",
        "                prev_year = yearly_counts.index[i-1]\n",
        "                prev_count = yearly_counts.iloc[i-1]\n",
        "                change = count - prev_count\n",
        "                pct_change = (change / prev_count) * 100\n",
        "                trend_indicator = \"üìà\" if change > 0 else \"üìâ\" if change < 0 else \"‚û°Ô∏è\"\n",
        "                print(f\"  {year}: {count:,} complaints {trend_indicator} ({change:+,} = {pct_change:+.1f}%)\")\n",
        "            else:\n",
        "                print(f\"  {year}: {count:,} complaints (baseline)\")\n",
        "    \n",
        "    # Monthly seasonal patterns\n",
        "    print(\"\\nüìÖ SEASONAL PATTERNS:\")\n",
        "    print(\"=\"*30)\n",
        "    monthly_avg = df_with_dates.groupby('month').size()\n",
        "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    \n",
        "    print(\"Average complaints by month:\")\n",
        "    overall_monthly_avg = monthly_avg.mean()\n",
        "    for month, avg_count in monthly_avg.items():\n",
        "        month_name = month_names[month-1]\n",
        "        deviation = ((avg_count - overall_monthly_avg) / overall_monthly_avg) * 100\n",
        "        trend_indicator = \"üî•\" if deviation > 20 else \"‚ùÑÔ∏è\" if deviation < -20 else \"üå°Ô∏è\"\n",
        "        print(f\"  {month_name}: {avg_count:.1f} {trend_indicator} ({deviation:+.1f}% vs avg)\")\n",
        "    \n",
        "    # Weekday patterns\n",
        "    print(\"\\nüìä WEEKDAY PATTERNS:\")\n",
        "    print(\"=\"*30)\n",
        "    weekday_counts = df_with_dates['weekday'].value_counts()\n",
        "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    weekday_avg = weekday_counts.mean()\n",
        "    \n",
        "    for day in weekday_order:\n",
        "        if day in weekday_counts:\n",
        "            count = weekday_counts[day]\n",
        "            deviation = ((count - weekday_avg) / weekday_avg) * 100\n",
        "            business_indicator = \"üíº\" if day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] else \"üè°\"\n",
        "            print(f\"  {day}: {count} {business_indicator} ({deviation:+.1f}% vs avg)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No suitable date column found for trend analysis\")\n",
        "    print(\"Performing categorical frequency analysis instead...\")\n",
        "    \n",
        "    # Alternative analysis when no dates available\n",
        "    for col in categorical_cols:\n",
        "        if df[col].nunique() < 20:  # Only analyze columns with reasonable number of unique values\n",
        "            print(f\"\\nüìä {col.upper()} FREQUENCY ANALYSIS:\")\n",
        "            value_counts = df[col].value_counts().head(10)\n",
        "            total = len(df)\n",
        "            for value, count in value_counts.items():\n",
        "                percentage = (count / total) * 100\n",
        "                print(f\"  {str(value)[:50]}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# Growth rate calculations\n",
        "if main_date_col and len(yearly_counts) > 1:\n",
        "    print(\"\\nüìà GROWTH RATE ANALYSIS:\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    # Calculate year-over-year growth\n",
        "    growth_rates = []\n",
        "    for i in range(1, len(yearly_counts)):\n",
        "        current_year = yearly_counts.index[i]\n",
        "        prev_year = yearly_counts.index[i-1]\n",
        "        current_count = yearly_counts.iloc[i]\n",
        "        prev_count = yearly_counts.iloc[i-1]\n",
        "        \n",
        "        yoy_growth = ((current_count - prev_count) / prev_count) * 100\n",
        "        growth_rates.append(yoy_growth)\n",
        "        \n",
        "        print(f\"  {prev_year} ‚Üí {current_year}: {yoy_growth:+.1f}% growth\")\n",
        "    \n",
        "    if growth_rates:\n",
        "        avg_growth = np.mean(growth_rates)\n",
        "        print(f\"\\n  Average annual growth rate: {avg_growth:.1f}%\")\n",
        "        \n",
        "        if avg_growth > 10:\n",
        "            print(\"  üö® HIGH GROWTH TREND detected!\")\n",
        "        elif avg_growth < -10:\n",
        "            print(\"  üìâ DECLINING TREND detected!\")\n",
        "        else:\n",
        "            print(\"  ‚û°Ô∏è STABLE TREND detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: EXTREME VALUE ANALYSIS\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 3: EXTREME VALUE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Function to find and analyze extreme values\n",
        "def analyze_extreme_values(df, column, analysis_type=\"both\"):\n",
        "    \"\"\"Analyze extreme values following journalism rules\"\"\"\n",
        "    \n",
        "    if df[column].dtype == 'object':\n",
        "        # For categorical data, find most/least frequent\n",
        "        value_counts = df[column].value_counts()\n",
        "        print(f\"\\nüîç EXTREME VALUES IN {column.upper()}:\")\n",
        "        print(\"=\"*40)\n",
        "        \n",
        "        if analysis_type in [\"both\", \"max\"]:\n",
        "            print(\"üìä MOST FREQUENT VALUES:\")\n",
        "            for i, (value, count) in enumerate(value_counts.head(5).items()):\n",
        "                percentage = (count / len(df)) * 100\n",
        "                print(f\"  {i+1}. {str(value)[:50]}: {count:,} occurrences ({percentage:.1f}%)\")\n",
        "                \n",
        "        if analysis_type in [\"both\", \"min\"] and len(value_counts) > 5:\n",
        "            print(\"\\nüìä LEAST FREQUENT VALUES:\")\n",
        "            for i, (value, count) in enumerate(value_counts.tail(5).items()):\n",
        "                percentage = (count / len(df)) * 100\n",
        "                print(f\"  {i+1}. {str(value)[:50]}: {count:,} occurrences ({percentage:.1f}%)\")\n",
        "        \n",
        "        return value_counts.head(1).index[0], value_counts.head(1).values[0]\n",
        "    \n",
        "    else:\n",
        "        # For numerical data\n",
        "        col_data = df[column].dropna()\n",
        "        if len(col_data) == 0:\n",
        "            return None, None\n",
        "            \n",
        "        mean_val = col_data.mean()\n",
        "        std_val = col_data.std()\n",
        "        \n",
        "        print(f\"\\nüîç EXTREME VALUES IN {column.upper()}:\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"Mean: {mean_val:.2f}, Std Dev: {std_val:.2f}\")\n",
        "        \n",
        "        if analysis_type in [\"both\", \"max\"]:\n",
        "            max_val = col_data.max()\n",
        "            max_idx = col_data.idxmax()\n",
        "            std_from_mean = (max_val - mean_val) / std_val if std_val > 0 else 0\n",
        "            \n",
        "            print(f\"\\nüìà MAXIMUM VALUE: {max_val}\")\n",
        "            print(f\"  Standard deviations from mean: {std_from_mean:.2f}\")\n",
        "            print(f\"  Percentage above mean: {((max_val - mean_val) / mean_val * 100):.1f}%\")\n",
        "            \n",
        "        if analysis_type in [\"both\", \"min\"]:\n",
        "            min_val = col_data.min()\n",
        "            min_idx = col_data.idxmin()\n",
        "            std_from_mean = (mean_val - min_val) / std_val if std_val > 0 else 0\n",
        "            \n",
        "            print(f\"\\nüìâ MINIMUM VALUE: {min_val}\")\n",
        "            print(f\"  Standard deviations from mean: {std_from_mean:.2f}\")\n",
        "            print(f\"  Percentage below mean: {((mean_val - min_val) / mean_val * 100):.1f}%\")\n",
        "        \n",
        "        return max_val if analysis_type != \"min\" else min_val, max_idx if analysis_type != \"min\" else min_idx\n",
        "\n",
        "# Look for location-related columns\n",
        "location_columns = []\n",
        "for col in df.columns:\n",
        "    col_lower = col.lower()\n",
        "    if any(keyword in col_lower for keyword in ['location', 'address', 'street', 'neighborhood', \n",
        "                                               'district', 'area', 'zone', 'community', 'precinct',\n",
        "                                               'council', 'ward', 'borough', 'city', 'zip', 'postal']):\n",
        "        location_columns.append(col)\n",
        "\n",
        "print(\"üó∫Ô∏è LOCATION COLUMNS IDENTIFIED:\")\n",
        "print(f\"Found {len(location_columns)} location-related columns: {location_columns}\")\n",
        "\n",
        "# Analyze each location column for extremes\n",
        "extreme_locations = {}\n",
        "\n",
        "for col in location_columns:\n",
        "    extreme_value, extreme_count = analyze_extreme_values(df, col, \"max\")\n",
        "    if extreme_value is not None:\n",
        "        extreme_locations[col] = {\n",
        "            'value': extreme_value,\n",
        "            'count': extreme_count,\n",
        "            'column': col\n",
        "        }\n",
        "\n",
        "# Find outliers using IQR method for numerical columns\n",
        "print(\"\\nüéØ STATISTICAL OUTLIERS ANALYSIS:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "outlier_indices = set()\n",
        "for col in numerical_cols:\n",
        "    col_data = df[col].dropna()\n",
        "    if len(col_data) > 0:\n",
        "        Q1 = col_data.quantile(0.25)\n",
        "        Q3 = col_data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
        "        if len(outliers) > 0:\n",
        "            print(f\"\\n{col}: {len(outliers)} outliers found\")\n",
        "            print(f\"  Normal range: {lower_bound:.2f} to {upper_bound:.2f}\")\n",
        "            print(f\"  Outlier values: {sorted(outliers.values)}\")\n",
        "            outlier_indices.update(outliers.index)\n",
        "\n",
        "if outlier_indices:\n",
        "    print(f\"\\nTotal records with outliers: {len(outlier_indices)}\")\n",
        "else:\n",
        "    print(\"No statistical outliers found in numerical columns.\")\n",
        "\n",
        "# Analyze all categorical columns for frequency-based extremes\n",
        "print(\"\\nüìç COMPREHENSIVE LOCATION ANALYSIS:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "all_location_analysis = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if 'location' in col.lower() or 'address' in col.lower() or any(loc_word in col.lower() \n",
        "                                                                    for loc_word in ['street', 'neighborhood', 'area', 'district']):\n",
        "        value_counts = df[col].value_counts()\n",
        "        if len(value_counts) > 0:\n",
        "            most_frequent = value_counts.index[0]\n",
        "            frequency = value_counts.iloc[0]\n",
        "            \n",
        "            all_location_analysis[col] = {\n",
        "                'most_frequent_location': most_frequent,\n",
        "                'complaint_count': frequency,\n",
        "                'percentage': (frequency / len(df)) * 100,\n",
        "                'total_unique_locations': len(value_counts)\n",
        "            }\n",
        "            \n",
        "            print(f\"\\nüèÜ {col.upper()} - TOP COMPLAINT LOCATION:\")\n",
        "            print(f\"  Location: {most_frequent}\")\n",
        "            print(f\"  Complaints: {frequency:,}\")\n",
        "            print(f\"  Percentage of total: {(frequency / len(df)) * 100:.1f}%\")\n",
        "            print(f\"  Total unique locations: {len(value_counts):,}\")\n",
        "            \n",
        "            # Show top 5 locations\n",
        "            print(f\"  Top 5 locations in {col}:\")\n",
        "            for i, (location, count) in enumerate(value_counts.head(5).items(), 1):\n",
        "                pct = (count / len(df)) * 100\n",
        "                print(f\"    {i}. {str(location)[:60]}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Store the extreme location for detailed analysis\n",
        "if all_location_analysis:\n",
        "    # Find the location with the highest complaint count across all location columns\n",
        "    extreme_location_info = max(all_location_analysis.items(), \n",
        "                               key=lambda x: x[1]['complaint_count'])\n",
        "    \n",
        "    extreme_column = extreme_location_info[0]\n",
        "    extreme_data = extreme_location_info[1]\n",
        "    \n",
        "    print(f\"\\nüö® EXTREME LOCATION IDENTIFIED:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Column: {extreme_column}\")\n",
        "    print(f\"Location: {extreme_data['most_frequent_location']}\")\n",
        "    print(f\"Complaint Count: {extreme_data['complaint_count']:,}\")\n",
        "    print(f\"Percentage: {extreme_data['percentage']:.1f}%\")\n",
        "    \n",
        "    # Store for detailed analysis in next step\n",
        "    EXTREME_LOCATION = extreme_data['most_frequent_location']\n",
        "    EXTREME_COLUMN = extreme_column\n",
        "    EXTREME_COUNT = extreme_data['complaint_count']\n",
        "    \n",
        "    print(f\"\\n‚úÖ Will perform detailed analysis on: {EXTREME_LOCATION}\")\n",
        "else:\n",
        "    print(\"‚ùå No clear location columns found for extreme analysis\")\n",
        "    EXTREME_LOCATION = None\n",
        "    EXTREME_COLUMN = None\n",
        "    EXTREME_COUNT = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: DETAILED EXTREME LOCATION ANALYSIS\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: DETAILED EXTREME LOCATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if EXTREME_LOCATION and EXTREME_COLUMN:\n",
        "    print(f\"üéØ DEEP DIVE ANALYSIS: {EXTREME_LOCATION}\")\n",
        "    print(f\"üìä Column: {EXTREME_COLUMN}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Filter data for the extreme location\n",
        "    extreme_location_data = df[df[EXTREME_COLUMN] == EXTREME_LOCATION].copy()\n",
        "    \n",
        "    print(f\"üìà BASIC STATISTICS:\")\n",
        "    print(f\"  Total complaints at this location: {len(extreme_location_data):,}\")\n",
        "    print(f\"  Percentage of all complaints: {(len(extreme_location_data) / len(df)) * 100:.2f}%\")\n",
        "    \n",
        "    # Calculate how extreme this is\n",
        "    all_location_counts = df[EXTREME_COLUMN].value_counts()\n",
        "    mean_complaints = all_location_counts.mean()\n",
        "    std_complaints = all_location_counts.std()\n",
        "    \n",
        "    if std_complaints > 0:\n",
        "        std_deviations = (EXTREME_COUNT - mean_complaints) / std_complaints\n",
        "        print(f\"  Standard deviations above mean: {std_deviations:.2f}\")\n",
        "        \n",
        "        if std_deviations > 3:\n",
        "            print(\"  üö® EXTREMELY UNUSUAL - Beyond 3 standard deviations!\")\n",
        "        elif std_deviations > 2:\n",
        "            print(\"  ‚ö†Ô∏è  VERY UNUSUAL - Beyond 2 standard deviations!\")\n",
        "        elif std_deviations > 1:\n",
        "            print(\"  üìä MODERATELY UNUSUAL - Beyond 1 standard deviation\")\n",
        "    \n",
        "    print(f\"  Average complaints per location: {mean_complaints:.1f}\")\n",
        "    print(f\"  This location vs average: {(EXTREME_COUNT / mean_complaints):.1f}x higher\")\n",
        "    \n",
        "    # Show comparison with next closest locations\n",
        "    print(f\"\\nüìä COMPARISON WITH OTHER TOP LOCATIONS:\")\n",
        "    print(\"=\"*50)\n",
        "    top_5_locations = all_location_counts.head(5)\n",
        "    for i, (location, count) in enumerate(top_5_locations.items(), 1):\n",
        "        percentage = (count / len(df)) * 100\n",
        "        is_extreme = \"üéØ\" if location == EXTREME_LOCATION else \"  \"\n",
        "        print(f\"{is_extreme} {i}. {str(location)[:50]}\")\n",
        "        print(f\"     Complaints: {count:,} ({percentage:.1f}%)\")\n",
        "        if i == 1 and len(top_5_locations) > 1:\n",
        "            diff_to_second = count - top_5_locations.iloc[1]\n",
        "            print(f\"     Gap to #2: {diff_to_second:,} complaints ({(diff_to_second/top_5_locations.iloc[1]*100):.1f}% more)\")\n",
        "    \n",
        "    # Complete record analysis - show all data for this extreme location\n",
        "    print(f\"\\nüìã COMPLETE RECORDS FOR {EXTREME_LOCATION}:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Showing all {len(extreme_location_data)} records:\")\n",
        "    \n",
        "    # Display first 10 complete records\n",
        "    display_count = min(10, len(extreme_location_data))\n",
        "    for i in range(display_count):\n",
        "        record = extreme_location_data.iloc[i]\n",
        "        print(f\"\\nüîç RECORD {i+1}:\")\n",
        "        for col, value in record.items():\n",
        "            if pd.notna(value):\n",
        "                print(f\"  {col}: {value}\")\n",
        "    \n",
        "    if len(extreme_location_data) > display_count:\n",
        "        print(f\"\\n... and {len(extreme_location_data) - display_count} more records\")\n",
        "    \n",
        "    # Temporal analysis for this location if dates available\n",
        "    if main_date_col and main_date_col in extreme_location_data.columns:\n",
        "        extreme_with_dates = extreme_location_data[extreme_location_data[main_date_col].notna()]\n",
        "        \n",
        "        if len(extreme_with_dates) > 0:\n",
        "            print(f\"\\nüìÖ TEMPORAL ANALYSIS FOR {EXTREME_LOCATION}:\")\n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            date_range = extreme_with_dates[main_date_col]\n",
        "            print(f\"Date range: {date_range.min()} to {date_range.max()}\")\n",
        "            print(f\"Time span: {(date_range.max() - date_range.min()).days} days\")\n",
        "            \n",
        "            # Yearly breakdown for this location\n",
        "            yearly_breakdown = extreme_with_dates.groupby(extreme_with_dates[main_date_col].dt.year).size()\n",
        "            if len(yearly_breakdown) > 0:\n",
        "                print(\"\\nComplaints by year:\")\n",
        "                for year, count in yearly_breakdown.items():\n",
        "                    print(f\"  {year}: {count:,} complaints\")\n",
        "                \n",
        "                # Calculate trend for this location\n",
        "                if len(yearly_breakdown) > 1:\n",
        "                    years = list(yearly_breakdown.index)\n",
        "                    counts = list(yearly_breakdown.values)\n",
        "                    trend_change = counts[-1] - counts[0]\n",
        "                    trend_pct = (trend_change / counts[0]) * 100 if counts[0] > 0 else 0\n",
        "                    \n",
        "                    print(f\"\\nTrend: {trend_change:+,} complaints ({trend_pct:+.1f}%) from {years[0]} to {years[-1]}\")\n",
        "                    \n",
        "                    if trend_pct > 50:\n",
        "                        print(\"  üö® DRAMATIC INCREASE at this location!\")\n",
        "                    elif trend_pct > 20:\n",
        "                        print(\"  üìà SIGNIFICANT INCREASE at this location\")\n",
        "                    elif trend_pct < -50:\n",
        "                        print(\"  üìâ DRAMATIC DECREASE at this location\")\n",
        "                    elif trend_pct < -20:\n",
        "                        print(\"  üìâ SIGNIFICANT DECREASE at this location\")\n",
        "    \n",
        "    # Analyze patterns in other columns for this extreme location\n",
        "    print(f\"\\nüîç PATTERN ANALYSIS FOR {EXTREME_LOCATION}:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if col != EXTREME_COLUMN and extreme_location_data[col].nunique() > 1:\n",
        "            if df[col].dtype == 'object':\n",
        "                value_counts = extreme_location_data[col].value_counts().head(5)\n",
        "                if len(value_counts) > 0:\n",
        "                    print(f\"\\n{col} patterns:\")\n",
        "                    for value, count in value_counts.items():\n",
        "                        pct = (count / len(extreme_location_data)) * 100\n",
        "                        print(f\"  {str(value)[:40]}: {count} ({pct:.1f}%)\")\n",
        "            \n",
        "            elif df[col].dtype in ['int64', 'float64']:\n",
        "                col_stats = extreme_location_data[col].describe()\n",
        "                print(f\"\\n{col} statistics:\")\n",
        "                print(f\"  Mean: {col_stats['mean']:.2f}\")\n",
        "                print(f\"  Range: {col_stats['min']:.2f} to {col_stats['max']:.2f}\")\n",
        "    \n",
        "    # Potential causes analysis\n",
        "    print(f\"\\nüí° POTENTIAL STORY ANGLES FOR {EXTREME_LOCATION}:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"üîç Possible explanations for high complaint volume:\")\n",
        "    print(\"  ‚Ä¢ High foot traffic area (business district, transit hub)\")\n",
        "    print(\"  ‚Ä¢ Aging infrastructure requiring frequent repairs\")\n",
        "    print(\"  ‚Ä¢ Accessibility-focused community advocacy\")\n",
        "    print(\"  ‚Ä¢ Poor initial construction requiring repeated fixes\")\n",
        "    print(\"  ‚Ä¢ Area with high disability/senior population\")\n",
        "    print(\"  ‚Ä¢ Tourist/visitor area with higher visibility\")\n",
        "    print(\"  ‚Ä¢ Recent development or construction impacts\")\n",
        "    \n",
        "    print(f\"\\nüì∞ JOURNALISTIC ANGLES TO EXPLORE:\")\n",
        "    print(\"  ‚Ä¢ Interview residents/business owners about accessibility challenges\")\n",
        "    print(\"  ‚Ä¢ Check city budget allocated to this area for ramp repairs\")\n",
        "    print(\"  ‚Ä¢ Compare complaint resolution times to other areas\") \n",
        "    print(\"  ‚Ä¢ Investigate if complaints correlate with specific events/seasons\")\n",
        "    print(\"  ‚Ä¢ Examine demographic data for disability/accessibility needs\")\n",
        "    print(\"  ‚Ä¢ Follow up on completed vs pending complaints\")\n",
        "    \n",
        "    print(f\"\\nüìä RECOMMENDED FOLLOW-UP DATA:\")\n",
        "    print(\"  ‚Ä¢ City budget data for infrastructure repairs\")\n",
        "    print(\"  ‚Ä¢ Demographic data (age, disability status) for this area\")\n",
        "    print(\"  ‚Ä¢ Complaint resolution times and completion rates\")\n",
        "    print(\"  ‚Ä¢ Construction/development permits in the area\")\n",
        "    print(\"  ‚Ä¢ Public transit usage data\")\n",
        "    print(\"  ‚Ä¢ Similar data from other cities for comparison\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No extreme location identified for detailed analysis\")\n",
        "    print(\"This could mean:\")\n",
        "    print(\"  ‚Ä¢ No clear location columns found in the dataset\")\n",
        "    print(\"  ‚Ä¢ Complaints are evenly distributed across locations\")\n",
        "    print(\"  ‚Ä¢ Location data may need cleaning or standardization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: STORY INSIGHTS SUMMARY\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: NEWS STORY INSIGHTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Collect all potential stories\n",
        "story_insights = []\n",
        "\n",
        "# Story 1: Extreme Location Story\n",
        "if EXTREME_LOCATION and EXTREME_COLUMN:\n",
        "    extreme_percentage = (EXTREME_COUNT / len(df)) * 100\n",
        "    \n",
        "    story_insights.append({\n",
        "        'headline': f\"Single Location Accounts for {extreme_percentage:.1f}% of All Pedestrian Ramp Complaints\",\n",
        "        'type': 'Extreme',\n",
        "        'key_finding': f\"{EXTREME_LOCATION} has generated {EXTREME_COUNT:,} complaints, far exceeding other locations\",\n",
        "        'primary_metric': EXTREME_COUNT,\n",
        "        'comparison': f\"Average location has {df[EXTREME_COLUMN].value_counts().mean():.1f} complaints\",\n",
        "        'significance': f\"{std_deviations:.1f} standard deviations above average\" if 'std_deviations' in locals() else \"Highly unusual concentration\",\n",
        "        'angle': \"Infrastructure inequality and accessibility hotspots in the city\"\n",
        "    })\n",
        "\n",
        "# Story 2: Temporal Trends\n",
        "if main_date_col and 'yearly_counts' in locals() and len(yearly_counts) > 1:\n",
        "    if 'avg_growth' in locals():\n",
        "        if abs(avg_growth) > 10:\n",
        "            trend_type = \"surge\" if avg_growth > 0 else \"decline\"\n",
        "            story_insights.append({\n",
        "                'headline': f\"Pedestrian Ramp Complaints Show {avg_growth:+.1f}% Annual {trend_type.title()}\",\n",
        "                'type': 'Trend',\n",
        "                'key_finding': f\"Complaints have been {'increasing' if avg_growth > 0 else 'decreasing'} by an average of {abs(avg_growth):.1f}% per year\",\n",
        "                'primary_metric': f\"{avg_growth:+.1f}% annual growth\",\n",
        "                'comparison': f\"Total change from {yearly_counts.index[0]} to {yearly_counts.index[-1]}: {yearly_counts.iloc[-1] - yearly_counts.iloc[0]:+,} complaints\",\n",
        "                'significance': \"Major trend requiring attention\",\n",
        "                'angle': \"City infrastructure maintenance and accessibility planning effectiveness\"\n",
        "            })\n",
        "\n",
        "# Story 3: Distribution Inequality\n",
        "if len(location_columns) > 0 and EXTREME_COLUMN:\n",
        "    location_counts = df[EXTREME_COLUMN].value_counts()\n",
        "    # Calculate inequality metrics\n",
        "    top_10_pct = location_counts.head(int(len(location_counts) * 0.1)).sum()\n",
        "    total_complaints = len(df)\n",
        "    top_10_pct_share = (top_10_pct / total_complaints) * 100\n",
        "    \n",
        "    if top_10_pct_share > 50:  # If top 10% of locations account for >50% of complaints\n",
        "        story_insights.append({\n",
        "            'headline': f\"Top 10% of Locations Account for {top_10_pct_share:.1f}% of All Ramp Complaints\",\n",
        "            'type': 'Inequality',\n",
        "            'key_finding': f\"Complaints are heavily concentrated in a small number of locations\",\n",
        "            'primary_metric': f\"{top_10_pct_share:.1f}% concentration\",\n",
        "            'comparison': f\"Should be ~10% if evenly distributed\",\n",
        "            'significance': \"Major geographic inequality in accessibility issues\",\n",
        "            'angle': \"Systematic infrastructure neglect in specific areas\"\n",
        "        })\n",
        "\n",
        "# Story 4: Seasonal Patterns\n",
        "if main_date_col and 'monthly_avg' in locals():\n",
        "    max_month = monthly_avg.idxmax()\n",
        "    min_month = monthly_avg.idxmin()\n",
        "    seasonal_difference = ((monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean()) * 100\n",
        "    \n",
        "    if seasonal_difference > 30:  # Significant seasonal variation\n",
        "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "        peak_month_name = month_names[max_month - 1]\n",
        "        low_month_name = month_names[min_month - 1]\n",
        "        \n",
        "        story_insights.append({\n",
        "            'headline': f\"Pedestrian Ramp Complaints Peak in {peak_month_name}, Drop {seasonal_difference:.1f}% in {low_month_name}\",\n",
        "            'type': 'Seasonal',\n",
        "            'key_finding': f\"Clear seasonal pattern with {peak_month_name} seeing most complaints and {low_month_name} the fewest\",\n",
        "            'primary_metric': f\"{seasonal_difference:.1f}% seasonal variation\",\n",
        "            'comparison': f\"{peak_month_name}: {monthly_avg.max():.1f} avg vs {low_month_name}: {monthly_avg.min():.1f} avg\",\n",
        "            'significance': \"Strong seasonal correlation suggests weather or usage pattern factors\",\n",
        "            'angle': \"Weather impacts on accessibility infrastructure and complaint filing patterns\"\n",
        "        })\n",
        "\n",
        "# Print all story insights in journalism format\n",
        "print(\"üì∞ POTENTIAL NEWS STORIES IDENTIFIED:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, story in enumerate(story_insights, 1):\n",
        "    print(f\"\\nüóûÔ∏è  STORY INSIGHT #{i}:\")\n",
        "    print(f\"HEADLINE: {story['headline']}\")\n",
        "    print(f\"Type: {story['type']}\")\n",
        "    print(f\"Key Finding: {story['key_finding']}\")\n",
        "    print(\"Supporting Data:\")\n",
        "    print(f\"  - Primary metric: {story['primary_metric']}\")\n",
        "    if 'time_period' in story:\n",
        "        print(f\"  - Time period: {story['time_period']}\")\n",
        "    print(f\"  - Comparison: {story['comparison']}\")\n",
        "    print(f\"Statistical Significance: {story['significance']}\")\n",
        "    print(f\"Potential Angle: {story['angle']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Overall Data Quality Assessment\n",
        "print(f\"\\nüìä DATA QUALITY ASSESSMENT:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"‚úÖ Records analyzed: {len(df):,}\")\n",
        "print(f\"‚úÖ Columns analyzed: {len(df.columns)}\")\n",
        "print(f\"‚úÖ Location columns identified: {len(location_columns)}\")\n",
        "print(f\"‚úÖ Temporal analysis: {'Possible' if main_date_col else 'Limited'}\")\n",
        "\n",
        "if len(story_insights) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  LIMITED STORY POTENTIAL:\")\n",
        "    print(\"Possible reasons:\")\n",
        "    print(\"  ‚Ä¢ Data may be too evenly distributed\")\n",
        "    print(\"  ‚Ä¢ Time period covered may be too short\")\n",
        "    print(\"  ‚Ä¢ Additional context data needed\")\n",
        "    print(\"  ‚Ä¢ May require data cleaning/preprocessing\")\n",
        "\n",
        "print(f\"\\nüéØ ANALYSIS COMPLETE!\")\n",
        "print(f\"Found {len(story_insights)} potential news stories\")\n",
        "print(\"Ready for further investigation and follow-up reporting\")\n",
        "\n",
        "# Create simple visualization if matplotlib is available\n",
        "try:\n",
        "    if EXTREME_LOCATION and EXTREME_COLUMN:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Top 10 locations bar chart\n",
        "        top_locations = df[EXTREME_COLUMN].value_counts().head(10)\n",
        "        \n",
        "        plt.subplot(2, 1, 1)\n",
        "        bars = plt.bar(range(len(top_locations)), top_locations.values)\n",
        "        bars[0].set_color('red')  # Highlight the extreme location\n",
        "        plt.title(f'Top 10 Locations by Complaint Count\\n(Red bar shows extreme location: {EXTREME_LOCATION})')\n",
        "        plt.ylabel('Number of Complaints')\n",
        "        plt.xticks(range(len(top_locations)), [str(x)[:20] + '...' if len(str(x)) > 20 else str(x) \n",
        "                                              for x in top_locations.index], rotation=45, ha='right')\n",
        "        \n",
        "        # Time series if available\n",
        "        if main_date_col and 'monthly_counts' in locals():\n",
        "            plt.subplot(2, 1, 2)\n",
        "            monthly_counts.plot(kind='line', marker='o')\n",
        "            plt.title('Complaints Over Time')\n",
        "            plt.ylabel('Number of Complaints')\n",
        "            plt.xlabel('Time Period')\n",
        "            plt.xticks(rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\\nüìà Visualization generated above\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\nüìä Visualization skipped: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AI DATA JOURNALIST ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
